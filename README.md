# ğŸ‡ºğŸ‡¦ Ukraine Tweets Sentiment Analysis Pipeline

A complete end-to-end data pipeline for sentiment analysis of Ukraine-related tweets using Apache Airflow, Spark, PostgreSQL, Druid, and Superset.

## ğŸ“ Project Structure

```
ukraine-tweets-sentiment-analysis/
â”œâ”€â”€ ğŸ“‚ airflow/                    # Airflow configuration and DAGs
â”‚   â”œâ”€â”€ dags/                      # DAG definitions
â”‚   â”œâ”€â”€ logs/                      # Airflow execution logs
â”‚   â”œâ”€â”€ plugins/                   # Custom Airflow plugins
â”‚   â”œâ”€â”€ Dockerfile                 # Airflow container image
â”‚   â””â”€â”€ requirements.txt           # Python dependencies
â”‚
â”œâ”€â”€ ğŸ“‚ spark/                      # Spark job definitions
â”‚   â”œâ”€â”€ sentiment_analysis.py     # Main sentiment analysis job
â”‚   â”œâ”€â”€ Dockerfile                 # Spark container image
â”‚   â””â”€â”€ requirements.txt           # Spark dependencies
â”‚
â”œâ”€â”€ ğŸ“‚ superset/                   # Apache Superset configuration
â”‚   â”œâ”€â”€ dashboards/                # Dashboard definitions
â”‚   â”œâ”€â”€ create_dashboard.py       # Dashboard setup script
â”‚   â”œâ”€â”€ init_superset.sh          # Initialization script
â”‚   â””â”€â”€ Dockerfile                 # Superset container image
â”‚
â”œâ”€â”€ ğŸ“‚ openmetadata/               # OpenMetadata integration
â”‚   â”œâ”€â”€ config.py                  # Configuration
â”‚   â””â”€â”€ init_openmetadata.sh      # Setup script
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                    # Setup and utility scripts
â”‚   â”œâ”€â”€ init-databases.sh         # Database initialization
â”‚   â”œâ”€â”€ setup_airflow_connections.sh   # Airflow connections
â”‚   â”œâ”€â”€ setup_airflow_connections.bat  # Windows setup
â”‚   â””â”€â”€ query_druid.sh            # Druid query utility
â”‚
â”œâ”€â”€ ğŸ“‚ data/                       # Data storage
â”‚   â”œâ”€â”€ raw/                       # Raw tweet data
â”‚   â”‚   â”œâ”€â”€ ukraine_tweets.csv    # Full dataset
â”‚   â”‚   â””â”€â”€ ukraine_tweets_sample_100.csv  # Test sample
â”‚   â”œâ”€â”€ processed/                 # Processed results
â”‚   â”‚   â””â”€â”€ sentiment_results/    # Analysis output
â”‚   â””â”€â”€ druid_ingestion_spec.json # Druid ingestion config
â”‚
â”œâ”€â”€ ğŸ“‚ tools/                      # Utility tools
â”‚   â”œâ”€â”€ ğŸ“‚ data_preparation/       # Data sampling and preparation
â”‚   â”‚   â”œâ”€â”€ create_sample_100.py   # Create 100-row sample
â”‚   â”‚   â”œâ”€â”€ create_sample_dataset.py # Create larger samples
â”‚   â”‚   â”œâ”€â”€ create_mock_data.py    # Generate mock data
â”‚   â”‚   â””â”€â”€ download_dataset.py    # Download from source
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ database_loaders/       # Database loading scripts
â”‚   â”‚   â”œâ”€â”€ load_to_postgres_sqlalchemy.py  # âœ… Main loader (recommended)
â”‚   â”‚   â”œâ”€â”€ load_to_postgres_direct.py      # Direct psycopg2 method
â”‚   â”‚   â”œâ”€â”€ load_to_postgres_fixed.py       # Alternative method
â”‚   â”‚   â”œâ”€â”€ load_to_postgres_simple.py      # Simple COPY method
â”‚   â”‚   â””â”€â”€ load_results_to_postgres.py     # Legacy loader
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ diagnostics/            # Monitoring and debugging
â”‚       â”œâ”€â”€ verify_postgres.py     # Check PostgreSQL data
â”‚       â”œâ”€â”€ check_druid_data.py    # Check Druid data
â”‚       â”œâ”€â”€ diagnose_druid_connection.py  # Druid diagnostics
â”‚       â”œâ”€â”€ view_results.py        # View analysis results
â”‚       â””â”€â”€ monitor_pipeline.py    # Pipeline monitoring
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                       # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md            # System architecture
â”‚   â”œâ”€â”€ PROJECT_STRUCTURE.md       # Project organization
â”‚   â”œâ”€â”€ QUICKSTART.md             # Quick start guide
â”‚   â”œâ”€â”€ TROUBLESHOOTING.md        # Common issues and fixes
â”‚   â”œâ”€â”€ SUPERSET_CONNECTION_GUIDE.md  # Superset setup (â­ Start here!)
â”‚   â”œâ”€â”€ SUPERSET_SETUP.md         # Detailed Superset guide
â”‚   â”œâ”€â”€ CONNECT_SUPERSET_TO_DRUID.md  # Druid connection
â”‚   â”œâ”€â”€ VISUAL_OVERVIEW.md        # Visual diagrams
â”‚   â”œâ”€â”€ INDEX.md                  # Documentation index
â”‚   â”œâ”€â”€ SUMMARY.md                # Project summary
â”‚   â””â”€â”€ CHECKLIST.md              # Implementation checklist
â”‚
â”œâ”€â”€ ğŸ“‚ config/                     # Configuration files
â”‚   â””â”€â”€ generate_keys.py          # Generate encryption keys
â”‚
â”œâ”€â”€ ğŸ“„ docker-compose.yml          # Docker services definition
â”œâ”€â”€ ğŸ“„ Makefile                    # Build and run commands
â”œâ”€â”€ ğŸ“„ .env.example                # Example environment config
â”œâ”€â”€ ğŸ“„ setup.sh                    # Linux/Mac setup script
â”œâ”€â”€ ğŸ“„ SETUP.bat                   # Windows setup script
â””â”€â”€ ğŸ“„ README.md                   # This file
```

## ğŸš€ Quick Start

### Prerequisites

-   Docker Desktop (Windows/Mac) or Docker Engine (Linux)
-   8GB+ RAM available for Docker
-   10GB+ disk space
-   Python 3.11+ (optional, for data preparation)

### Step 1: Clone and Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/ukraine-tweets-sentiment-analysis.git
cd ukraine-tweets-sentiment-analysis

# Copy environment template
cp .env.example .env
```

### Step 2: Download Dataset

**Option A: Full Dataset (1.2M rows, 44GB)**

1. Go to [Kaggle Dataset](https://www.kaggle.com/datasets/bwandowando/ukraine-russian-crisis-twitter-dataset-1-2-m-rows)
2. Download CSV and place in `data/raw/ukraine_tweets.csv`

**Option B: Quick Test (100 rows) - Recommended for first run**

```bash
python tools/data_preparation/create_sample_100.py
```

### Step 3: Start Services

**Linux/Mac:**

```bash
./setup.sh
docker-compose up -d
```

**Windows:**

```powershell
.\SETUP.bat
docker-compose up -d
```

Wait 2-3 minutes for all services to initialize.

### Step 4: Run the Pipeline

1. **Access Airflow UI**: http://localhost:8080

    - Username: `airflow`, Password: `airflow`

2. **Enable the DAG**: Find `twitter_sentiment_pipeline` and toggle it on

3. **Trigger the DAG**: Click the play button (â–¶ï¸) to run manually

4. **Monitor progress**: Pipeline completes in ~30 seconds for 100 rows

### Step 5: Load Results to Database

```bash
# Load results to PostgreSQL
python tools/database_loaders/load_to_postgres_sqlalchemy.py

# Verify data loaded successfully
python tools/diagnostics/verify_postgres.py
```

### Step 6: Visualize in Superset

1. **Open Superset**: http://localhost:8088

    - Username: `admin`, Password: `admin`

2. **Follow the detailed guide**: [`docs/SUPERSET_CONNECTION_GUIDE.md`](docs/SUPERSET_CONNECTION_GUIDE.md)

3. **Quick setup**:

    - Go to: Settings â†’ Database Connections â†’ + Database
    - Select: PostgreSQL
    - URI: `postgresql://airflow:airflow@sentiment-postgres:5432/airflow`
    - Test â†’ Connect

4. Create dataset from table: `ukraine_tweets_sentiment` and build dashboards!

## ğŸ”§ Service URLs

| Service           | URL                   | Credentials       |
| ----------------- | --------------------- | ----------------- |
| **Airflow**       | http://localhost:8080 | airflow / airflow |
| **Spark Master**  | http://localhost:8081 | -                 |
| **Druid Console** | http://localhost:8888 | -                 |
| **Superset**      | http://localhost:8088 | admin / admin     |
| **PostgreSQL**    | localhost:5432        | airflow / airflow |

## ğŸ“Š Pipeline Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw CSV   â”‚â”€â”€â”€â”€â–¶â”‚   Airflow    â”‚â”€â”€â”€â”€â–¶â”‚    Spark    â”‚â”€â”€â”€â”€â–¶â”‚  PostgreSQL  â”‚
â”‚  (Tweets)   â”‚     â”‚ Orchestrator â”‚     â”‚  Sentiment  â”‚     â”‚   (Results)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  Analysis   â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
                                                                      â–¼
                                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                              â”‚   Superset   â”‚
                                                              â”‚ (Dashboards) â”‚
                                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pipeline Steps:

1. **Data Ingestion**: Load raw tweet CSV
2. **Preprocessing**: Clean text, remove special characters
3. **Sentiment Analysis**: Spark + DistilBERT model (Hugging Face)
4. **Storage**: Save to PostgreSQL
5. **Visualization**: Create dashboards in Superset

### Tech Stack:

-   **Orchestration**: Apache Airflow 2.x
-   **Processing**: Apache Spark 3.x (Standalone cluster)
-   **ML Model**: DistilBERT (transformers library)
-   **Storage**: PostgreSQL 14, Apache Druid 28
-   **Visualization**: Apache Superset 3.x
-   **Containerization**: Docker Compose

4. **Monitor Progress**: Watch the DAG execution in the Graph view

The pipeline will:

-   âœ… Validate the input dataset
-   âœ… Run Spark sentiment analysis (using Hugging Face Transformers)
-   âœ… Save processed results
-   âœ… Ingest data into Druid
-   âœ… Log metadata to PostgreSQL

### Step 7: Create Superset Dashboards

Once data is in Druid, create visualizations:

```bash
# Run dashboard creation script
docker exec -it sentiment-superset python /app/dashboards/create_dashboard.py
```

Or create manually:

1. Go to http://localhost:8088
2. Login with admin/admin
3. Navigate to Databases â†’ Add Database
4. Select Druid and configure: `druid://druid-broker:8082/druid/v2/sql/`
5. Create charts and dashboards from the `ukraine_tweets_sentiment` datasource

### Step 8: Configure OpenMetadata (Optional)

1. Access OpenMetadata: http://localhost:8585
2. Login with admin/admin
3. Add services (Settings â†’ Services):
    - Airflow Pipeline Service
    - Druid Database Service
    - Superset Dashboard Service
    - PostgreSQL Database Service
4. Run metadata ingestion to track data lineage

## ğŸ“ Project Structure

```
ukraine-tweets-sentiment-analysis/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ twitter_sentiment_dag.py    # Main orchestration DAG
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ sentiment_analysis.py           # Spark processing script
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ superset/
â”‚   â”œâ”€â”€ create_dashboard.py             # Dashboard automation
â”‚   â”œâ”€â”€ init_superset.sh               # Initialization script
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ openmetadata/
â”‚   â”œâ”€â”€ config.py                       # OpenMetadata connectors
â”‚   â””â”€â”€ init_openmetadata.sh
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ init-databases.sh              # PostgreSQL setup
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                           # Input CSV files
â”‚   â””â”€â”€ processed/                     # Spark output
â”œâ”€â”€ docker-compose.yml                 # Main orchestration file
â”œâ”€â”€ .env.example                       # Environment template
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸ”§ Pipeline Details

### Spark Sentiment Analysis

The `spark/sentiment_analysis.py` script:

1. **Data Loading**: Reads CSV from Kaggle dataset
2. **Text Cleaning**:
    - Removes URLs, mentions (@user)
    - Strips hashtags (keeps text)
    - Removes special characters
    - Normalizes whitespace
3. **Sentiment Analysis**:
    - Uses Hugging Face `distilbert-base-uncased-finetuned-sst-2-english` model
    - Classifies tweets as POSITIVE, NEGATIVE, or NEUTRAL
    - Processes in batches for efficiency
4. **Output**: Saves CSV with sentiment column

### Airflow DAG

The pipeline DAG (`airflow/dags/twitter_sentiment_dag.py`) includes:

```
check_data â†’ create_output_dir â†’ create_metadata_table
    â†“
spark_sentiment_analysis
    â†“
validate_output
    â†“
prepare_druid_spec
    â†“
submit_to_druid
    â†“
log_metadata
    â†“
success_notification
```

**Schedule**: Daily (`@daily`)

### Druid Ingestion

Data is ingested into Druid with:

-   **Datasource**: `ukraine_tweets_sentiment`
-   **Timestamp**: `tweetcreatedts`
-   **Dimensions**: userid, username, location, text, sentiment, hashtags
-   **Metrics**: count, total_followers, total_retweets
-   **Granularity**: DAY (segments), HOUR (queries)

### Superset Dashboards

Pre-configured visualizations:

1. **Sentiment Over Time** - Line chart showing sentiment trends
2. **Sentiment Distribution** - Pie chart of POSITIVE/NEGATIVE/NEUTRAL
3. **Top Locations** - Bar chart of most active locations
4. **Sentiment by Location** - Heatmap of location vs sentiment

## ğŸ› ï¸ Development

### Rebuild Services

```bash
# Rebuild specific service
docker-compose build spark-master

# Rebuild all services
docker-compose build
```

### View Logs

```bash
# View all logs
docker-compose logs -f

# View specific service logs
docker-compose logs -f airflow-webserver
docker-compose logs -f spark-master
```

### Stop Services

```bash
# Stop all services
docker-compose down

# Stop and remove volumes (clean slate)
docker-compose down -v
```

### Execute Commands in Containers

```bash
# Spark submit manually
docker exec -it sentiment-spark-master spark-submit \
  --master spark://spark-master:7077 \
  /opt/spark-apps/sentiment_analysis.py \
  /opt/spark-data/raw/ukraine_tweets.csv \
  /opt/spark-data/processed/sentiment_results

# Access PostgreSQL
docker exec -it sentiment-postgres psql -U airflow -d airflow

# Access Airflow CLI
docker exec -it sentiment-airflow-webserver airflow dags list
```

## ğŸ” Monitoring

### Check Service Health

```bash
# Check running containers
docker ps

# Check resource usage
docker stats

# Check Airflow scheduler health
curl http://localhost:8080/health

# Check Spark master status
curl http://localhost:8081
```

### Common Issues

**Issue**: Airflow webserver won't start

-   **Solution**: Check Fernet key is set in `.env`

**Issue**: Spark job fails with memory error

-   **Solution**: Increase Docker Desktop memory limit to 8GB+

**Issue**: Druid ingestion fails

-   **Solution**: Ensure data directory is mounted correctly in docker-compose

**Issue**: Services can't communicate

-   **Solution**: Verify all services are on `sentiment-network`

## ğŸ“ˆ Scaling

### Process Larger Datasets

1. **Add Spark Workers**:

```yaml
# In docker-compose.yml, duplicate spark-worker service
spark-worker-2:
    # ... same config as spark-worker
```

2. **Increase Resources**:

```yaml
spark-worker:
    environment:
        SPARK_WORKER_MEMORY: 8G
        SPARK_WORKER_CORES: 4
```

3. **Partition Data**:

```python
# In sentiment_analysis.py
df.repartition(10).write.csv(output_path)
```

## ğŸ§ª Testing

### Test Spark Script Locally

```bash
docker exec -it sentiment-spark-master bash
cd /opt/spark-apps
python sentiment_analysis.py /opt/spark-data/raw/sample.csv /opt/spark-data/test
```

### Test Druid Query

```bash
curl -X POST 'http://localhost:8888/druid/v2/sql' \
  -H 'Content-Type: application/json' \
  -d '{"query":"SELECT sentiment, COUNT(*) as count FROM ukraine_tweets_sentiment GROUP BY sentiment"}'
```

## ğŸ“š Additional Resources

-   [Apache Airflow Documentation](https://airflow.apache.org/docs/)
-   [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
-   [Apache Druid Documentation](https://druid.apache.org/docs/latest/design/)
-   [Apache Superset Documentation](https://superset.apache.org/docs/intro)
-   [OpenMetadata Documentation](https://docs.open-metadata.org/)
-   [Hugging Face Transformers](https://huggingface.co/docs/transformers/)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

## ğŸ“ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

-   Kaggle for the Ukraine-Russia crisis Twitter dataset
-   Hugging Face for sentiment analysis models
-   Apache Software Foundation for open-source tools
-   OpenMetadata community

## ğŸ“§ Contact

For questions or issues, please open a GitHub issue or contact the maintainers.

---

**Note**: This pipeline processes real Twitter data about a sensitive geopolitical event. Please use responsibly and consider ethical implications when analyzing and sharing sentiment results.
